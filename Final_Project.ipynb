{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final-Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats\n",
        "from scipy.stats import skew\n",
        "from scipy.special import boxcox1p\n",
        "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV \n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error,  mean_absolute_error, mean_absolute_percentage_error,r2_score\n",
        "import statsmodels.api as sm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "39jTtJIMQay9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd0c32d-34b2-474b-9de1-b11a7fb034a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q shap\n",
        "import shap"
      ],
      "metadata": {
        "id": "O_LwjY9nQwW4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "properties=pd.read_csv(\"ames.csv\")"
      ],
      "metadata": {
        "id": "JT-wQ1ZaiOY5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Null data and summary"
      ],
      "metadata": {
        "id": "zbv0Vw58rISo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head = properties.head()\n",
        "shape = properties.shape\n",
        "describe = properties.describe().T\n",
        "print(shape)\n",
        "print(head)"
      ],
      "metadata": {
        "id": "0Pda99MIiOc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "properties.describe().T"
      ],
      "metadata": {
        "id": "JpysVcG9iTA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tells how many values are present in each column and how many %of zeroes are there\n",
        "def data_summaries (df):\n",
        "    for col in df.columns:\n",
        "        print (\"__%s__\" % col,'\\n')\n",
        "        if df[col].dtype == np.object:\n",
        "            print (df[col].value_counts())\n",
        "        elif len(df[df[col] == 0]) >= 1465:\n",
        "            print ('% zeros (HIGH!): ', round((len(df[df[col] == 0])/2930)*100),\\\n",
        "                   '\\n','no. zeros: ',len(df[df[col]==0]))\n",
        "        else:\n",
        "            print ('% zeros: ', round((len(df[df[col] == 0])/2930)*100))\n",
        "        print ('\\n')\n",
        "    \n",
        "data_summaries(properties)"
      ],
      "metadata": {
        "id": "m0FXL0HPiVnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "properties.columns[properties.isnull().any()]"
      ],
      "metadata": {
        "id": "fjmBmJtwLuq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tells the null value present in the dataset in descending order\n",
        "null_data = []\n",
        "for item in properties.isnull().sum().items():\n",
        "    null_data.append([item[0], int(item[1])])\n",
        "\n",
        "null_data = np.array(null_data, dtype=np.object)\n",
        "null_data = null_data[np.argsort(null_data[:, 1])][::-1]\n",
        "null_data"
      ],
      "metadata": {
        "id": "rLD93V7wiWR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this handles the null values in the dataset\n",
        "# if the column is float, it imputes with the mean\n",
        "# if the column is categorical, then depending on the column type\n",
        "# it imputes with either the mode or a seperate '0'`value\n",
        "# if it is expected that nulis the default, then it imputes using the '0'`- e.g. null means no pool\n",
        "# whereas if it is expected that null means a missing value, then it is replaced with the mode\n",
        "\n",
        "impute_with_na = ['Pool.QC', \n",
        "                 'Misc.Feature',\n",
        "                 'Alley',\n",
        "                 'Fence',\n",
        "                 'Fireplace.Qu',\n",
        "                 ]\n",
        "\n",
        "impute_with_mode = ['Garage.Qual',\n",
        "                    'Garage.Cond',\n",
        "                    'Garage.Finish',\n",
        "                    'Garage.Type',\n",
        "                    'Bsmt.Exposure',\n",
        "                    'BsmtFin.Type.2',\n",
        "                    'BsmtFin.Type.1',\n",
        "                    'Bsmt.Cond',\n",
        "                    'Bsmt.Qual',\n",
        "                    'Mas.Vnr.Type',\n",
        "                    'Electrical',\n",
        "                    ]\n",
        "properties_processed = properties.copy()\n",
        "for row in null_data:\n",
        "  if row[1]>0:\n",
        "    if properties[row[0]].dtype==float:\n",
        "        properties_processed[row[0]] = properties_processed[row[0]].fillna(properties_processed[row[0]].mean())\n",
        "        continue\n",
        "\n",
        "    if row[0] in impute_with_na:\n",
        "        properties_processed[row[0]].loc[properties_processed[row[0]].isnull()]='Na'\n",
        "        continue\n",
        "    if row[0] in impute_with_mode:\n",
        "        properties_processed[row[0]].loc[properties_processed[row[0]].isnull()]=properties_processed[row[0]].mode().values[0]\n",
        "        continue\n",
        "    raise f\"Column {row[0]} not handled\"\n",
        "    \n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "RQZWLMrGrsow"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checks if there are any null values present \n",
        "properties_processed.columns[properties_processed.isnull().any()]"
      ],
      "metadata": {
        "id": "InJJwNNNYw3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All nominal and ordinal features with their values\n",
        "cat = properties_processed.select_dtypes(include=['object'])\n",
        "for (colname,colval) in cat.iteritems():\n",
        "    print(colname, colval.unique())"
      ],
      "metadata": {
        "id": "ADXpNWHbY_Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_numerical = [col for col in properties_processed.columns if properties_processed[col].dtype != np.object]\n",
        "col_cat = [col for col in properties_processed.columns if properties_processed[col].dtype == np.object]"
      ],
      "metadata": {
        "id": "gyAQ_4nhaLgr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "HSYGsVN9rAPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To check for skewness of the data in ordinal/nominal columns\n",
        "\n",
        "nominal_stats = {'column': [], 'most_frequent_value': [], 'percentage_occurrence':[], 'column_status': []}\n",
        "\n",
        "threshold = 90.0 # threshold value for considering the column to be severely skewed towards the mode value\n",
        "\n",
        "for col in col_cat:\n",
        "    nominal_stats['column'].append(col)\n",
        "    mode = properties_processed.loc[:,col].mode()[0]\n",
        "    nominal_stats['most_frequent_value'].append(mode)\n",
        "    proportion = np.round(properties_processed.loc[properties_processed.loc[:,col]==properties_processed.loc[:,col].mode()[0],:].shape[0] / properties_processed.shape[0] * 100, 1)\n",
        "    nominal_stats['percentage_occurrence'].append(proportion)\n",
        "    nominal_stats['column_status'].append('' if proportion < threshold else 'Severely skewed')\n",
        "\n",
        "nominal_stats_df = pd.DataFrame(nominal_stats)\n",
        "nominal_stats_df"
      ],
      "metadata": {
        "id": "T6127gP85m3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To check for skewness of the data in cont/discrete columns\n",
        "\n",
        "nominal_stats = {'column': [], 'most_frequent_value': [], 'percentage_occurrence':[], 'column_status': []}\n",
        "\n",
        "threshold = 90.0 # threshold value for considering the column to be severely skewed towards the mode value\n",
        "\n",
        "for col in col_numerical:\n",
        "    nominal_stats['column'].append(col)\n",
        "    mode = properties_processed.loc[:,col].mode()[0]\n",
        "    nominal_stats['most_frequent_value'].append(mode)\n",
        "    proportion = np.round(properties_processed.loc[properties_processed.loc[:,col]==properties_processed.loc[:,col].mode()[0],:].shape[0] / properties_processed.shape[0] * 100, 1)\n",
        "    nominal_stats['percentage_occurrence'].append(proportion)\n",
        "    nominal_stats['column_status'].append('' if proportion < threshold else 'Severely skewed')\n",
        "\n",
        "nominal_stats_df = pd.DataFrame(nominal_stats)\n",
        "nominal_stats_df"
      ],
      "metadata": {
        "id": "aGn6-en0C9hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram subplots\n",
        "def subplot_histograms(dataframe, list_of_columns):\n",
        "    nrows = int(np.ceil(len(list_of_columns)/4)) \n",
        "    fig, ax = plt.subplots(nrows=nrows, ncols=4,figsize=(20,nrows*4)) \n",
        "    ax = ax.ravel() \n",
        "    for i, column in enumerate(list_of_columns): \n",
        "        ax[i].hist(dataframe[column],bins=15)\n",
        "        ax[i].set_title(f'{column} distribution',fontsize=17)\n",
        "        ax[i].tick_params(labelsize=15)\n",
        "        ax[i].set_xlabel(column, fontsize=15)\n",
        "        ax[i].set_ylabel(f'{column} values', fontsize=15)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "# scatterplot subplots\n",
        "def subplot_scatter(dataframe, list_of_columns):\n",
        "    nrows = int(np.ceil(len(list_of_columns)/4)) \n",
        "    fig, ax = plt.subplots(nrows=nrows, ncols=4,figsize=(20, nrows*4)) \n",
        "    ax = ax.ravel() \n",
        "    for i, column in enumerate(list_of_columns): \n",
        "        sns.regplot(y=dataframe.SalePrice, x=dataframe[column],ax=ax[i], \n",
        "                    scatter_kws={'facecolors':'lightgreen','edgecolor':'lightgreen'},\n",
        "                    line_kws = {'color':'red'})\n",
        "        ax[i].set_title(f'{column} vs SalePrice',fontsize=15)  \n",
        "        ax[i].tick_params(labelsize=15)\n",
        "        ax[i].set_xlabel(column, fontsize=15)\n",
        "        ax[i].set_ylabel('SalePrice', fontsize=15) \n",
        "    plt.tight_layout()\n",
        "    \n",
        "# boxplot subplots\n",
        "def subplot_box(dataframe, list_of_columns):\n",
        "    nrows = int(np.ceil(len(list_of_columns)/4)) \n",
        "    fig, ax = plt.subplots(nrows=nrows, ncols=4,figsize=(20, nrows*4)) \n",
        "    ax = ax.ravel() \n",
        "    for i, column in enumerate(list_of_columns): \n",
        "        sns.boxplot(x = dataframe[column], y = dataframe.SalePrice, width = 0.5, ax = ax[i], color='lightgreen')\n",
        "        ax[i].set_title(column,fontsize=15)  \n",
        "        ax[i].tick_params(labelsize=15)\n",
        "        ax[i].set_xlabel(column, fontsize=15)\n",
        "        ax[i].set_ylabel('SalePrice', fontsize=15)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "# distribution plots (histogram, boxplot, probplot)\n",
        "def dist_plots(df, list_of_columns):\n",
        "    nrows = len(list_of_columns)\n",
        "    fig, ax = plt.subplots(nrows = nrows, ncols = 3, figsize=(20, nrows*4))\n",
        "    ax = ax.ravel()\n",
        "    for i, col in enumerate(list_of_columns):\n",
        "        sns.distplot(df[col], ax = ax[i*3-3], fit = stats.norm)\n",
        "        ax[i*3-3].set_title(f'{col} distribution plot',fontsize=15)\n",
        "        ax[i*3-3].tick_params(labelsize=15)\n",
        "        ax[i*3-3].set_xlabel(col, fontsize=15)\n",
        "        \n",
        "        sns.boxplot(df[col], width = 0.2, ax = ax[i*3-2])\n",
        "        ax[i*3-2].set_title(f'{col} box plot',fontsize=15)\n",
        "        ax[i*3-2].tick_params(labelsize=15)\n",
        "        ax[i*3-2].set_xlabel(col, fontsize=15)\n",
        "        \n",
        "        stats.probplot(df[col], plot = ax[i*3-1])\n",
        "        ax[i*3-1].set_title(f'{col} probability plot', fontsize=15)\n",
        "        ax[i*3-1].tick_params(labelsize=15)\n",
        "        ax[i*3-1].set_xlabel(col, fontsize=15)\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "v-wZi03mZ627"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist_plots(properties_processed,col_numerical)"
      ],
      "metadata": {
        "id": "XATseQNoaAew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subplot_scatter(properties_processed,col_numerical)"
      ],
      "metadata": {
        "id": "3RLsmDOUaAht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subplot_histograms(properties_processed,col_cat)"
      ],
      "metadata": {
        "id": "xaSUO58VaAkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subplot_box(properties_processed, col_cat)"
      ],
      "metadata": {
        "id": "1MAl_35HaAnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# order columns\n",
        "properties_processed = properties_processed[properties_processed.columns.sort_values()]\n",
        "\n",
        "# plot heatmap\n",
        "mask = np.zeros_like(properties_processed.corr(), dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)]= True\n",
        "\n",
        "f, ax = plt.subplots(figsize=(40, 40))\n",
        "sns.heatmap(properties_processed.corr(), \n",
        "            annot=True,\n",
        "            mask = mask,\n",
        "            square=True,\n",
        "            vmin = -1,\n",
        "            vmax = 1,\n",
        "            linewidth=0.2,\n",
        "            cbar_kws = {'shrink':0.75},\n",
        "            cmap=sns.color_palette(\"husl\", 9),\n",
        "            annot_kws={'size': 13})\n",
        "ax.tick_params(labelsize=10)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "y_72QfcdaiMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot Encoding\n",
        "properties_processed_ohe = pd.get_dummies(properties_processed)"
      ],
      "metadata": {
        "id": "9blExLocalD6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get index of columns with 10 highest corrcoef with respect to saleprice\n",
        "correlation_coefficients = properties_processed_ohe.corr().nlargest(10, 'SalePrice').index\n",
        "\n",
        "# create heatmap\n",
        "mask = np.zeros_like(properties_processed_ohe[correlation_coefficients].corr(), dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)]= True\n",
        "\n",
        "f, ax = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(properties_processed_ohe[correlation_coefficients].corr(), \n",
        "            annot=True, \n",
        "            square= True, \n",
        "            mask = mask,\n",
        "            cmap=sns.color_palette(\"husl\", 9),\n",
        "            annot_kws={'size': 13},\n",
        "            cbar_kws={\"shrink\": 0.75},\n",
        "            linewidth = 0.1,\n",
        "            yticklabels=correlation_coefficients.values, \n",
        "            xticklabels=correlation_coefficients.values,\n",
        "            vmax = 1,\n",
        "            vmin = -1)\n",
        "ax.set_xlim(0,10)\n",
        "ax.set_ylim(0,10)\n",
        "ax.tick_params(labelsize=12)\n",
        "plt.title('Features with highest correlation coefficients', fontsize=19)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "_Nzpftofao2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 10 Values:\")\n",
        "numeric_data = properties_processed.select_dtypes(include=[np.number])\n",
        "corr = numeric_data.corr()\n",
        "print (corr['SalePrice'].sort_values(ascending=False)[:10], '\\n')\n",
        "print ('----------------------')\n",
        "print(\"Last 10 values:\")\n",
        "print (corr['SalePrice'].sort_values(ascending=False)[-10:])"
      ],
      "metadata": {
        "id": "Tv9i-TBYao4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(properties_processed_ohe[correlation_coefficients],size = 2 ,kind ='scatter',diag_kind='kde',corner=True)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "PBbyRcIoa5FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering "
      ],
      "metadata": {
        "id": "aAN0MrjXqz16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting ordinal columns with type object to int by assigning ranks. Using single mapper for all ordinal variables with their codes\n",
        "\n",
        "ordinal_mapper = {'Na': 0, 'Po': 1, 'Fa': 2, 'TA': 3,'Gd': 4,'Ex': 5 }\n",
        "properties_processed['Exter.Qual']    = properties_processed['Exter.Qual'].replace(ordinal_mapper)\n",
        "properties_processed['Exter.Cond']    = properties_processed['Exter.Cond'].replace(ordinal_mapper)\n",
        "properties_processed['Bsmt.Qual']     = properties_processed['Bsmt.Qual'].replace(ordinal_mapper)\n",
        "properties_processed['Bsmt.Cond']     = properties_processed['Bsmt.Cond'].replace(ordinal_mapper)\n",
        "properties_processed['Garage.Qual']   = properties_processed['Garage.Qual'].replace(ordinal_mapper)\n",
        "properties_processed['Garage.Cond']   = properties_processed['Garage.Cond'].replace(ordinal_mapper)\n",
        "properties_processed['Heating.QC']    = properties_processed['Heating.QC'].replace(ordinal_mapper)\n",
        "properties_processed['Kitchen.Qual']  = properties_processed['Kitchen.Qual'].replace(ordinal_mapper)\n",
        "properties_processed['Fireplace.Qu']  = properties_processed['Fireplace.Qu'].replace(ordinal_mapper)\n",
        "properties_processed['Pool.QC']       = properties_processed['Pool.QC'].replace(ordinal_mapper)\n",
        "\n",
        "properties_processed['Functional'] = properties_processed['Functional'].replace({'Sal': 8, 'Sev': 1, 'Maj2': 2, 'Maj1': 3,'Mod': 4,'Min2': 5, 'Min1': 6 ,'Typ': 7 })\n",
        "\n",
        "properties_processed['Central.Air'] = properties_processed['Central.Air'].map({'N': 1,'Y': 2})\n",
        "\n",
        "properties_processed['Fence'] = properties_processed['Fence'].map({'Na':0, 'MnPrv':1, 'GdPrv':2, 'GdWo':3, 'MnWw':4})\n",
        "\n",
        "properties_processed['MS.Zoning'] = properties_processed['MS.Zoning'].map({'RL':1, 'RH':2, 'FV':3, 'RM':4, 'C (all)':5, 'I (all)':6, 'A (agr)':7})\n",
        "\n",
        "properties_processed['Alley'] = properties_processed['Alley'].map({'Na':0 ,'Pave':1 ,'Grvl':2})\n",
        "\n",
        "properties_processed['Street'] = properties_processed['Street'].map({'Na':0 ,'Pave':1 ,'Grvl':2})\n",
        "\n",
        "properties_processed['Lot.Shape'] = properties_processed['Lot.Shape'].map({'IR1':1, 'Reg':2 ,'IR2':3, 'IR3':4})\n",
        "\n",
        "properties_processed['Land.Contour'] = properties_processed['Land.Contour'].map({'Lvl':1, 'HLS':2, 'Bnk':3, 'Low':4})\n",
        "\n",
        "properties_processed['Utilities'] = properties_processed['Utilities'].map({'AllPub':1, 'NoSewr':2, 'NoSeWa':3})\n",
        "\n",
        "properties_processed['Lot.Config'] = properties_processed['Lot.Config'].map({'Corner':1, 'Inside':2, 'CulDSac':3, 'FR2':4, 'FR3':5})\n",
        "\n",
        "properties_processed['Land.Slope'] = properties_processed['Land.Slope'].map({'Gtl':1, 'Mod':2, 'Sev':3})\n",
        "\n",
        "properties_processed['Condition.1'] = properties_processed['Condition.1'].map({'Norm':1, 'Feedr':2, 'PosN':3, 'RRNe':9, 'RRAe':5, 'Artery':6, 'PosA':7, 'RRAn':8, 'RRNn':4})\n",
        "\n",
        "properties_processed['Condition.2'] = properties_processed['Condition.2'].map({'Norm':1, 'Feedr':2, 'PosN':3,'RRAe':5, 'Artery':6, 'PosA':7, 'RRAn':8, 'RRNn':4})\n",
        "\n",
        "properties_processed['Bldg.Type'] = properties_processed['Bldg.Type'].map({'1Fam':1, 'TwnhsE':2, 'Twnhs':3, 'Duplex':4, '2fmCon':5})\n",
        "\n",
        "properties_processed['House.Style'] = properties_processed['House.Style'].map({'1Story':1, '2Story':2, '1.5Fin':3, 'SFoyer':4, 'SLvl':5, '2.5Unf':6, '1.5Unf':7, '2.5Fin':8})\n",
        "\n",
        "properties_processed['Roof.Style'] = properties_processed['Roof.Style'].map({'Hip':1, 'Gable':2, 'Mansard':3, 'Gambrel':4, 'Shed':5, 'Flat':6})\n",
        "\n",
        "properties_processed['Roof.Matl'] = properties_processed['Roof.Matl'].map({'CompShg':1, 'WdShake':2, 'Tar&Grv':3, 'WdShngl':4, 'Membran':5, 'ClyTile':6, 'Roll':7, 'Metal':8})\n",
        "\n",
        "properties_processed['Mas.Vnr.Type'] = properties_processed['Mas.Vnr.Type'].map({'Stone':1, 'None':2, 'BrkFace':3, 'BrkCmn':4, 'CBlock':5})\n",
        "\n",
        "properties_processed['Foundation'] = properties_processed['Foundation'].map({'CBlock':1, 'PConc':2, 'Wood':3, 'BrkTil':4, 'Slab':5, 'Stone':6})\n",
        "\n",
        "properties_processed['Heating'] = properties_processed['Heating'].map({'GasA':1, 'GasW':2, 'Grav':3, 'Wall':4, 'Floor':5, 'OthW':6})\n",
        "\n",
        "properties_processed['Electrical'] = properties_processed['Electrical'].map({'SBrkr':1, 'FuseA':2, 'FuseF':3, 'FuseP':4, 'Mix':5})\n",
        "\n",
        "properties_processed['Bsmt.Exposure'] = properties_processed['Bsmt.Exposure'].map({'Gd':1, 'No':2, 'Mn':3, 'Av':4})\n",
        "\n",
        "properties_processed['BsmtFin.Type.1'] = properties_processed['BsmtFin.Type.1'].map({'BLQ':1, 'Rec':2, 'ALQ':3, 'GLQ':4, 'Unf':5, 'LwQ':6})\n",
        "\n",
        "properties_processed['BsmtFin.Type.2'] = properties_processed['BsmtFin.Type.2'].map({'BLQ':1, 'Rec':2, 'ALQ':3, 'GLQ':4, 'Unf':5, 'LwQ':6})\n",
        "\n",
        "properties_processed['Garage.Type'] = properties_processed['Garage.Type'].map({'Attchd':1, 'BuiltIn':2, 'Basment':3, 'Detchd':4, 'CarPort':5, '2Types':6})\n",
        "\n",
        "properties_processed['Garage.Finish'] = properties_processed['Garage.Finish'].map({'Fin':1, 'Unf':2, 'RFn':3})\n",
        "\n",
        "properties_processed['Paved.Drive'] = properties_processed['Paved.Drive'].map({'P':1, 'Y':2, 'N':3})\n",
        "\n",
        "properties_processed['Misc.Feature'] = properties_processed['Misc.Feature'].map({'Na':0 ,'Gar2':1, 'Shed':2, 'Othr':3, 'Elev':4, 'TenC':5})\n",
        "\n",
        "properties_processed['Sale.Type'] = properties_processed['Sale.Type'].map({'WD ':1, 'New':2, 'COD':3, 'ConLI':4, 'Con':5, 'ConLD':6, 'Oth':7, 'ConLw':8, 'CWD':9, 'VWD':10})\n",
        "\n",
        "properties_processed['Sale.Condition'] = properties_processed['Sale.Condition'].map({'Normal':1, 'Partial':2, 'Family':3, 'Abnorml':4, 'Alloca':5, 'AdjLand':6})\n",
        "\n",
        "properties_processed['Exterior.1st'] = properties_processed['Exterior.1st'].map({'BrkFace':1, 'VinylSd':2,'Wd Sdng':3, 'CemntBd':4, 'HdBoard':5, 'Plywood':6, 'MetalSd':7,\n",
        "                                                                                     'AsbShng':8, 'WdShing':15, 'Stucco':9, 'AsphShn':10, 'BrkComm':16, 'CBlock':11, 'PreCast':12,\n",
        "                                                                                     'Stone':13, 'ImStucc':14})\n",
        "\n",
        "properties_processed['Exterior.2nd'] = properties_processed['Exterior.2nd'].map({'BrkFace':1, 'VinylSd':2,'Wd Sdng':3, 'CmentBd':4, 'HdBoard':5, 'Plywood':6, 'MetalSd':7,\n",
        "                                                                                     'AsbShng':8, 'Wd Shng':15, 'Stucco':9, 'AsphShn':10, 'Brk Cmn':16, 'CBlock':11, 'PreCast':12,\n",
        "                                                                                     'Stone':13, 'ImStucc':14, 'Other':17})"
      ],
      "metadata": {
        "id": "2xvn7rOAche0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "properties_processed['SalePrice'].groupby(properties_processed['Neighborhood']).median().sort_values().plot(kind='bar')"
      ],
      "metadata": {
        "id": "dPyl1KF1mRmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Depending on the above graph we can classify what region should we put the neighbourhoods in\n",
        "\n",
        "properties_processed['Neighborhood'] = properties_processed['Neighborhood'].map({'MeadowV':1, 'BrDale':1, 'IDOTRR':1,     \n",
        "'OldTown':2, 'Edwards':2, 'BrkSide':2, 'Blueste':2, 'Sawyer':2, 'SWISU':2, 'Landmrk':2, 'NAmes':2, 'NPkVill':2, 'Mitchel':2, \n",
        "'SawyerW':3, 'NWAmes':3, 'Gilbert':3, 'Blmngtn':3, 'ClearCr':3, 'Greens':3, 'CollgCr':3, 'Crawfor':3,                    \n",
        "'Somerst':4, 'Timber':4, 'Veenker':4, 'GrnHill':4, 'NoRidge':4, 'NridgHt':4, 'StoneBr':4})  "
      ],
      "metadata": {
        "id": "VFCBZrGAmRq2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "properties_processed['SalePrice'].groupby(properties_processed['Neighborhood']).median().sort_values().plot(kind='bar')"
      ],
      "metadata": {
        "id": "c8TVOkC0thRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cross check to see if any object features are present \n",
        "cat = properties_processed.select_dtypes(include=['object'])\n",
        "for (colname,colval) in cat.iteritems():\n",
        "    print(colname, colval.unique())"
      ],
      "metadata": {
        "id": "9-fseFEkhPef"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "properties_processed.columns"
      ],
      "metadata": {
        "id": "e_dT9IUO-abC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop columnns and outliers\n",
        "\n",
        "def process(dataframe):\n",
        "  df = dataframe.copy()\n",
        "\n",
        "  # function to drop columns\n",
        "  def dropcol(df, dropscol):\n",
        "   df.drop(dropscol, axis = 1, inplace = True)\n",
        "\n",
        "  # columns with >80% single category or >80% zeros, plus pid and order columns\n",
        "  dropcols = ['Order','PID','Street','Roof.Matl','Alley','Utilities','Condition.2','Heating','Garage.Cond','Misc.Val','Misc.Feature','X3Ssn.Porch',\n",
        "              'Bsmt.Half.Bath','Low.Qual.Fin.SF','Land.Slope','Garage.Yr.Blt','Garage.Qual',\n",
        "              ]\n",
        "  dropcol(df, dropcols)\n",
        "\n",
        "\n",
        "  #Outliers\n",
        "  df.drop(df[df['Lot.Frontage'] > 250].index, inplace = True)\n",
        "  df.drop(df[df['Lot.Area'] > 100000].index, inplace = True)\n",
        "  df.drop(df[df['Gr.Liv.Area'] > 4500].index, inplace = True)\n",
        "  df.drop(df[df['X1st.Flr.SF'] > 4000].index, inplace = True)\n",
        "  df.drop(df[df['Total.Bsmt.SF'] > 4000].index, inplace = True)\n",
        "  df.drop(df[df['BsmtFin.SF.1'] > 3000].index, inplace = True)\n",
        "\n",
        "  df=df.reset_index(drop=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "ZBtYt1_Sa-H9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fully_properties_processed=process(properties_processed)\n",
        "fully_properties_processed_ohe = pd.get_dummies(fully_properties_processed)"
      ],
      "metadata": {
        "id": "SoAmHmwqgFt-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(ytest, ypred):\n",
        "    return np.sqrt(mean_squared_error(ytest, ypred))"
      ],
      "metadata": {
        "id": "-GQBtNlkoZYL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest "
      ],
      "metadata": {
        "id": "DtArTBcXEP0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_RSS(X_rss, y_rss, attr, s):\n",
        "    d_1 = y_rss[X_rss[:, attr]<=s]\n",
        "    d_2 = y_rss[X_rss[:, attr]>s]\n",
        "    \n",
        "    if len(d_1)==0:\n",
        "        rss_1 = 0\n",
        "    else:\n",
        "        rss_1 = ((d_1 - d_1.mean())**2).sum()\n",
        "        \n",
        "    if len(d_2)==0:\n",
        "        rss_2 = 0\n",
        "    else:\n",
        "        rss_2 = ((d_2 - d_2.mean())**2).sum() \n",
        "    \n",
        "    return rss_1 + rss_2\n",
        "\n",
        "class TreeNode:\n",
        "    # treenode - automatically trains itself by building the tree beneath itself when initialised in a recursive manner\n",
        "    def __init__(self, X, y, depth_level, max_depth=3, features_sample=None):\n",
        "        self.depth_level = depth_level\n",
        "        self.max_depth   = max_depth\n",
        "        if features_sample:\n",
        "            self.features_sample = X.shape[1]\n",
        "        else:\n",
        "            self.features_sample = X.shape[1]/3\n",
        "            \n",
        "        if depth_level==max_depth or len(y)<=1:\n",
        "            self.type             = \"leaf\"\n",
        "            self.prediction_value = np.mean(y)\n",
        "            self.y                = y\n",
        "        else:\n",
        "            self.type  = \"split\"\n",
        "            \n",
        "            self.split_info = self.create_split(X_create_split=X, y_create_split=y)\n",
        "\n",
        "            left_indices  = X[:, self.split_info['best_attribute']]<=self.split_info['best_split_value']\n",
        "            right_indices = X[:, self.split_info['best_attribute']]>self.split_info['best_split_value']\n",
        "            \n",
        "            self.X_left = X[left_indices, :]\n",
        "            self.y_left = y[left_indices]\n",
        "\n",
        "            self.X_right = X[right_indices, :]\n",
        "            self.y_right = y[right_indices]\n",
        "            \n",
        "            if len(self.y_right)==0 or len(self.y_left)==0:\n",
        "                self.type             = \"leaf\"\n",
        "                self.prediction_value = np.mean(y)\n",
        "                self.y                = y\n",
        "            else:\n",
        "                self.create_lower_branches()\n",
        "    \n",
        "    # create lower branches - makes two new tree nodes below itself using the respective subsets of the data\n",
        "    def create_lower_branches(self):\n",
        "        self.left  = TreeNode(self.X_left, \n",
        "                              self.y_left, \n",
        "                              depth_level=self.depth_level+1,\n",
        "                              max_depth = self.max_depth)\n",
        "        self.right = TreeNode(self.X_right, \n",
        "                              self.y_right, \n",
        "                              depth_level=self.depth_level+1,\n",
        "                              max_depth = self.max_depth)\n",
        "    \n",
        "    # searches through the attributes and data to create the split\n",
        "    def create_split(self, X_create_split, y_create_split):\n",
        "        min_RSS = np.inf\n",
        "        size = int(np.round(self.features_sample))\n",
        "        attributes_considered = np.random.choice(X_create_split.shape[1], size=size, replace=False)\n",
        "        for attribute in attributes_considered:\n",
        "            unique_values = np.unique(X_create_split[:, attribute])\n",
        "            \n",
        "            for s in unique_values:\n",
        "                rss = calculate_RSS(X_create_split, y_create_split, attr=attribute, s=s)\n",
        "                if rss < min_RSS:\n",
        "                    min_RSS = rss\n",
        "                    best_attribute = attribute\n",
        "                    best_split_value = s\n",
        "        return {'min_RSS': min_RSS, \n",
        "                'best_attribute': best_attribute, \n",
        "                'best_split_value': best_split_value}\n",
        "    \n",
        "    # predicts for a single observation\n",
        "    # again works recursively, retrieving the prediction value of the relevant branch for each split\n",
        "    def predict(self,X):\n",
        "        if self.type==\"leaf\":\n",
        "\n",
        "            return self.prediction_value\n",
        "        else:\n",
        "            \n",
        "            if X[self.split_info['best_attribute']] <= self.split_info['best_split_value']:\n",
        "\n",
        "                return self.left.predict(X)\n",
        "            else:\n",
        "\n",
        "                return self.right.predict(X)\n",
        "    \n",
        "    # predicts for a group of observation\n",
        "    def predict_set(self, X):\n",
        "        return np.array([self.predict(x) for x in X])\n",
        "\n",
        "class RandomForest:\n",
        "    # the random forest uses a collection of trees, where each tree sees a boostrap of the data (selection with replacement)\n",
        "    # and each split sees a random subset of the features\n",
        "    \n",
        "    def __init__(self, number_trees=100, data_proportion_per_tree=0.66, max_depth=3):\n",
        "        self.number_trees = number_trees\n",
        "        self.data_proportion_per_tree = data_proportion_per_tree\n",
        "        self.amount_per_tree = int(data_proportion_per_tree*X_train.shape[0])\n",
        "        self.max_depth = max_depth\n",
        "    \n",
        "    # train by making the trees and store them\n",
        "    def train(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.number_trees):\n",
        "            indices_to_send = np.random.choice(X_train.shape[0], size = self.amount_per_tree, replace=True)\n",
        "            \n",
        "            # this initialisation of the tree automatically trains it\n",
        "            tree = TreeNode(X = X_train[indices_to_send], y = y_train[indices_to_send], depth_level=0, max_depth=self.max_depth)\n",
        "            self.trees.append(tree)\n",
        "            \n",
        "    # predict by predicting for each tree, then takng the mean\n",
        "    def predict(self, X):\n",
        "        return np.array([np.mean([tree.predict(x) for tree in self.trees]) for x in X])\n",
        "\n",
        "# function to get MSE quickly\n",
        "def get_MSE(y_predictions, y_labels):\n",
        "    return ((y_predictions-y_labels)**2).mean()\n",
        "\n",
        "def get_MRSE(y_predictions, y_labels):\n",
        "    return ((y_predictions-y_labels)**2).mean()**0.5"
      ],
      "metadata": {
        "id": "WghlRzTIVRfd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalising(data):\n",
        "    return (data-data.mean(axis=0))/data.std(axis=0)"
      ],
      "metadata": {
        "id": "4Hyeyjr7VnJh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_to_ignore = [\"SalePrice\"]\n",
        "target_column     = [\"SalePrice\"]\n",
        "\n",
        "X_columns = [column for column in fully_properties_processed_ohe.columns if column not in column_to_ignore]\n",
        "X = fully_properties_processed_ohe[X_columns].values\n",
        "y = fully_properties_processed_ohe[target_column].values\n",
        "y = y/100000\n",
        "\n",
        "X_normalised = normalising(X)\n",
        "X_normalised.std(axis=0)"
      ],
      "metadata": {
        "id": "qzP31KAXVRjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_normalised, y, test_size=0.3, random_state=2022)"
      ],
      "metadata": {
        "id": "0ifcmWdoVxKi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForest(number_trees=100, data_proportion_per_tree=0.7, max_depth=10)\n",
        "rf.train(X_train, y_train)\n",
        "predictions = rf.predict(X_train)\n",
        "train_MSE = get_MSE(predictions, y_train)\n",
        "train_MRSE = get_MRSE(predictions, y_train)\n",
        "predictions = rf.predict(X_test)\n",
        "test_MSE  = get_MSE(predictions, y_test)\n",
        "test_MRSE  = get_MRSE(predictions, y_test)\n",
        "print(f\"Train MSE: {train_MSE}, Train MRSE: {train_MRSE}\")\n",
        "print(f\"Test MSE: {test_MSE}, Test MRSE: {test_MRSE}\")"
      ],
      "metadata": {
        "id": "DsfJWQyDVRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso"
      ],
      "metadata": {
        "id": "xstJbCuHHlqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler()\n",
        "Z_train = sc.fit_transform(X_train)\n",
        "Z_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "KhMDac6GH07U"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a list of Lasso alphas to check.\n",
        "l_alphas = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
        "\n",
        "# Cross-validate over our list of Lasso alphas.\n",
        "lasso_cv = LassoCV(alphas=l_alphas, cv=5, max_iter=5000)\n",
        "\n",
        "# Fit model using best ridge alpha!\n",
        "lasso_model = lasso_cv.fit(Z_train, y_train);"
      ],
      "metadata": {
        "id": "D4hRcGM3H09_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train score:', lasso_model.score(Z_train, y_train))\n",
        "print(f'Test score:', lasso_model.score(Z_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-26sWVYH1As",
        "outputId": "da823d7d-ba38-4597-e7b7-a0a1f63f0a35"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train score: 0.9064229737975885\n",
            "Test score: 0.8886827550959064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute predictions on test data\n",
        "preds_l = lasso_model.predict(Z_test)\n",
        "log_preds_l = np.log(preds_l)"
      ],
      "metadata": {
        "id": "ocsE7XDrH1F-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_test, preds_l)"
      ],
      "metadata": {
        "id": "EwSZP-uCIHBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(y_test, preds_l, c='crimson')\n",
        "p1 = max(max(preds_l), max(y_test))\n",
        "p2 = min(min(preds_l), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('True Values', fontsize=15)\n",
        "plt.ylabel('Predictions', fontsize=15)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g0ZaoKqbu-cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.explainers.Linear(lasso_cv, Z_test)\n",
        "shap_values = explainer.shap_values(Z_test)\n",
        "shap.summary_plot(shap_values, Z_test)"
      ],
      "metadata": {
        "id": "L1zo6UX8H87f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.explainers.Linear(lasso_cv, Z_test)\n",
        "shap_values = explainer.shap_values(Z_test)\n",
        "shap.summary_plot(shap_values, Z_test, max_display=Z_test.shape[1])"
      ],
      "metadata": {
        "id": "nzN6Y5AfIHIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ridge"
      ],
      "metadata": {
        "id": "rT2jW3eaJxeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_alpha = np.logspace (0,5,200)\n",
        "\n",
        "# fits multiple alphas\n",
        "ridgecv = RidgeCV(alphas = r_alpha, cv = 5)\n",
        "ridgecv = ridgecv.fit(Z_train, y_train)\n",
        "\n",
        "print('optimal ridge alpha: ', ridgecv.alpha_)\n",
        "print('best ridge R2: ', ridgecv.score(Z_train, y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPgG-O6libVQ",
        "outputId": "fe4ed518-df07-4428-c972-34d8a846ce65"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimal ridge alpha:  2.3816855519761586\n",
            "best ridge R2:  0.9064137924185774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ridge = Ridge(alpha = ridgecv.alpha_)"
      ],
      "metadata": {
        "id": "6koYL1msibXt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_mod = ridge.fit(Z_train, y_train)\n",
        "# predict on test data\n",
        "preds_r = ridge_mod.predict(Z_test)\n",
        "# evaluate model performance\n",
        "print('ridge test R2: ', ridge_mod.score(Z_test, y_test))\n",
        "print('ridge test RMSE: ', rmse(y_test, preds_r))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVj1hQUlibaT",
        "outputId": "14b8d1e1-2279-4c34-e5b5-639ba40758b9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ridge test R2:  0.8887428828153559\n",
            "ridge test RMSE:  0.268416651372836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_test, preds_r)"
      ],
      "metadata": {
        "id": "5Go6DuBznpkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(y_test, preds_r, c='crimson')\n",
        "p1 = max(max(preds_r), max(y_test))\n",
        "p2 = min(min(preds_r), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('True Values', fontsize=15)\n",
        "plt.ylabel('Predictions', fontsize=15)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LSJPaqouu8v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.explainers.Linear(ridge_mod, Z_test)\n",
        "shap_values = explainer.shap_values(Z_test)\n",
        "shap.summary_plot(shap_values, Z_test)"
      ],
      "metadata": {
        "id": "nguX1eKLibfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.explainers.Linear(ridge_mod, Z_test)\n",
        "shap_values = explainer.shap_values(Z_test)\n",
        "shap.summary_plot(shap_values, Z_test, max_display=Z_test.shape[1])"
      ],
      "metadata": {
        "id": "FgWORWcaibdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Elastic Net"
      ],
      "metadata": {
        "id": "-PeyUPhbn3yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enet_alpha = np.arange(0, 1, 0.005)\n",
        "enet_ratio = [.01, .1, .2, .3, .5, .7, .9, .95, .99, 1]\n",
        "\n",
        "# fits multiple alphas and rhos\n",
        "enetcv = ElasticNetCV(alphas = enet_alpha, l1_ratio = enet_ratio, cv = 5)\n",
        "enetcv = enetcv.fit(Z_train, y_train)\n",
        "\n",
        "print('optimal enet alpha: ', enetcv.alpha_)\n",
        "print('optimal enet lambda: ', enetcv.l1_ratio_)\n",
        "print('best elastic net R2: ', enetcv.score(Z_train, y_train))"
      ],
      "metadata": {
        "id": "FUKKgz6Ribkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enet = ElasticNet(alpha = enetcv.alpha_, l1_ratio = enetcv.l1_ratio_)"
      ],
      "metadata": {
        "id": "cT7Z28Qzibng"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model to train data\n",
        "enet_mod = enet.fit(Z_train, y_train)\n",
        "# predict on test data\n",
        "preds_e = enet_mod.predict(Z_test)\n",
        "# evaluate model performance\n",
        "print('elastic net test R2: ', enet_mod.score(Z_test, y_test))\n",
        "print('elastic net test RMSE: ', rmse(y_test, preds_e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qtYGNXxibp3",
        "outputId": "96699154-a05b-40ef-a92f-a06ea241e382"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elastic net test R2:  0.889167645646346\n",
            "elastic net test RMSE:  0.2679037743123376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_test, preds_e)"
      ],
      "metadata": {
        "id": "rnAfIGKqqdMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(y_test, preds_e, c='crimson')\n",
        "p1 = max(max(preds_e), max(y_test))\n",
        "p2 = min(min(preds_e), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('True Values', fontsize=15)\n",
        "plt.ylabel('Predictions', fontsize=15)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VI7PB-rXutOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.explainers.Linear(enet_mod, Z_test)\n",
        "shap_values = explainer.shap_values(Z_test)\n",
        "shap.summary_plot(shap_values, Z_test)"
      ],
      "metadata": {
        "id": "agktPDFKibsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.explainers.Linear(enet_mod, Z_test)\n",
        "shap_values = explainer.shap_values(Z_test)\n",
        "shap.summary_plot(shap_values, Z_test, max_display=Z_test.shape[1])"
      ],
      "metadata": {
        "id": "ePrGFs42kLeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Visualizations which shows what are the features that affect the sales price"
      ],
      "metadata": {
        "id": "J85e5wFmw4EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coef_labels = [col for col in fully_properties_processed_ohe.columns if col != 'saleprice']\n",
        "lasso_coef = pd.DataFrame(lasso_model.coef_)             \n",
        "lasso_coef = lasso_coef[lasso_coef[0] != 0]                                  \n",
        "print(f'the model produced {lasso_coef.shape[0]} non-zero coefficients.')\n",
        "# sort\n",
        "lasso_coef = lasso_coef.reindex(lasso_coef[0].abs().sort_values(ascending=True).index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAVQ2HkIGECb",
        "outputId": "f6edd425-859e-4bd5-899c-2621adc8ee7b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the model produced 63 non-zero coefficients.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get list of top 20 predictors\n",
        "predictors = list(lasso_coef.index[-20:])\n",
        "len(predictors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPAQv4CQBD8W",
        "outputId": "3a469a48-4f43-4001-d60c-c4e73b0baaf3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final=[]\n",
        "for col in predictors:\n",
        "    final.append(fully_properties_processed.columns[col])"
      ],
      "metadata": {
        "id": "8hUjV_FkCCjF"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final"
      ],
      "metadata": {
        "id": "vEbAan8AKh5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subplot_scatter(fully_properties_processed,final)"
      ],
      "metadata": {
        "id": "wXLRDgxG_ZDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}